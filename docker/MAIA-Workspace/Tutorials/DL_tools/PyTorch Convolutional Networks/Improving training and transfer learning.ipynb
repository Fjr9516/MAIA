{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving training and transfer learning\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "Data augmentation is a technique used to increase the size of a training dataset by creating new training samples from the existing ones. This is particularly useful when the original dataset is small or when the model is not able to generalize well.\n",
    "\n",
    "There are several types of data augmentation techniques:\n",
    "\n",
    "1. **Image transformations**: This includes rotation, translation, scaling, flipping, and cropping.\n",
    "2. **Color manipulation**: Adjusting brightness, contrast, saturation, and hue.\n",
    "3. **Noise addition**: Adding random noise to the images.\n",
    "4. **Blurring**: Applying blurring filters to the images.\n",
    "5. **Distortion**: Distorting the images to create new samples.\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "An optimizer is an algorithm used to update the parameters of a model during training. The goal of an optimizer is to adjust the model's parameters to minimize the loss function.\n",
    "\n",
    "### SGD\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple and effective optimizer. It updates the model's parameters using the gradient of the loss function with respect to the parameters. The upda\n",
    "\n",
    "### Adam\n",
    "\n",
    "Adam is an optimizer that uses the gradient of the loss function with respect to the parameters. It differs from SGD in that it uses a more sophisticated update rule that takes into account the momentum and the second moment of the gradient.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Regularization is a technique used to prevent overfitting. It adds a penalty term to the loss function to discourage the model from fitting the training data too closely. This helps in improving the model's generalization ability.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique where randomly selected neurons are ignored during training. This helps in preventing the neurons from co-adapting.\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "L2 regularization adds a penalty term to the loss function to discourage the model from fitting the training data too closely. This helps in improving the model's generalization ability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.5719),(0.1684)) \n",
    "            ])\n",
    "\n",
    "# load medMNIST datset\n",
    "\n",
    "from medmnist import OrganSMNIST\n",
    "dataset_train = OrganSMNIST(root='utils/datasets', split='train', transform=transform, download=True)\n",
    "loader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_val = OrganSMNIST(root='utils/datasets', split='val', transform=transform, download=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_test = OrganSMNIST(root='utils/datasets', split='test', transform=transform, download=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "Here we will only work with the VGG model and see how we can improve its performance by using data augmentation, regularization and transfer learning.\n",
    "\n",
    "First let's train the VGG model and see how it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import vgg\n",
    "from utils.train import train, check_accuracy\n",
    "\n",
    "\n",
    "model = vgg.VGG13(num_classes=11, in_channels=1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model, results = train(model, loader_train, loader_val, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Check accuracy on the test set\n",
    "test_loss, test_accuracy = check_accuracy(model, loader_test, criterion, device)\n",
    "train_loss, train_accuracy = check_accuracy(model, loader_train, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the [torchvision documentation](https://pytorch.org/vision/0.19/) to find out how you can use data augmentation with torchvision. Then design an augmentation pipeline and apply it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "transform = v2.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.5719),(0.1684)) \n",
    "                # Add data augmentation here\n",
    "            ])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.5719),(0.1684)) \n",
    "                # No data augmentation here\n",
    "            ])\n",
    "# load medMNIST datset\n",
    "\n",
    "from medmnist import OrganSMNIST\n",
    "dataset_train_aug = OrganSMNIST(root='utils/datasets', split='train', transform=transform, download=True)\n",
    "loader_train_aug = DataLoader(dataset_train_aug, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_val_aug = OrganSMNIST(root='utils/datasets', split='val', transform=test_transform, download=True)\n",
    "loader_val_aug = DataLoader(dataset_val_aug, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_test = OrganSMNIST(root='utils/datasets', split='test', transform=test_transform, download=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the images before and after augmentation to see how it affects the images. This is an important step to ensure that the augmentation is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataset, title, start_index=0):\n",
    "    # Plot 10 images starting from start_index\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < 10:\n",
    "            # Get the image and its label\n",
    "            img, _ = dataset[start_index + i]\n",
    "            \n",
    "            # Denormalize the image\n",
    "            img = img.squeeze().cpu().numpy()\n",
    "            img = img * 0.1684 + 0.5719\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the augmented transform\n",
    "transform_aug = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomRotation(10),  # Rotate by up to 10 degrees\n",
    "    T.RandomAffine(0, translate=(0.1, 0.1)),  # Translate by up to 10%\n",
    "    T.Normalize((0.5719), (0.1684))\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "dataset_val = OrganSMNIST(root='utils/datasets', split='val', transform=transform, download=True)\n",
    "dataset_val_aug = OrganSMNIST(root='utils/datasets', split='val', transform=transform_aug, download=True)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plot original validation images\n",
    "plot_images(dataset_train, \"Original Validation Images\")\n",
    "\n",
    "# Plot augmented validation images\n",
    "plot_images(dataset_train_aug, \"Augmented Validation Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "What is an example of when a data augmentation technique may not be helpful or even harmful to the performance of a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import vgg\n",
    "from utils.train import train, check_accuracy\n",
    "\n",
    "\n",
    "model = vgg.VGG13(num_classes=11, in_channels=1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model, results = train(model, loader_train_aug, loader_val_aug, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Check accuracy on the test set\n",
    "test_loss, test_accuracy = check_accuracy(model, loader_test, criterion, device)\n",
    "train_loss, train_accuracy = check_accuracy(model, loader_train_aug, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "What differences do you observe in the training and validation loss curves when using data augmentation? Plot the training and validation loss curves for both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Let's compare the performance of SGD and Adam optimizer. We will use the same model and the same dataset as above. Implement SGD and Adam optimizer and train the model. Compare the training and validation loss curves for both optimizers.\n",
    "\n",
    "## Question\n",
    "What are the advantages and disadvantages of using SGD and Adam optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Training with SGD and plot the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Training with Adam and plot the training and validation loss curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "Transfer learning is a technique where a model trained on one task is reused as the starting point for training a model on a second, related task. This is useful when the second task has less data available than the first task.\n",
    "\n",
    "### Using a pre-trained model\n",
    "\n",
    "We will use the a model pre-trained on ImageNet and fine-tune it on the OrganSMNIST dataset.\n",
    "\n",
    "Look at the [torchvision documentation](https://pytorch.org/vision/stable/models.html) to find out how to load a pre-trained model.\n",
    "\n",
    "1. Try to train the model by training it from scratch and replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset.\n",
    "\n",
    "2. Load a pre-trainined model, freeze all the layers of the model except the last one and replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset.\n",
    "\n",
    "3. Load a pre-trained model, replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset and train the model on the OrganSMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training with a model from Torchvision from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = None # TODO: Load a model from torchvision.models and train it from scratch\n",
    "print(model)\n",
    "# Replace the first layer with a new layer that has the number of channels in the OrganSMNIST dataset e.g.\n",
    "#model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset e.g.\n",
    "#model.fc = nn.Linear(model.fc.in_features, 11)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model, results = train(model, loader_train_aug, loader_val_aug, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Check accuracy on the test set\n",
    "test_loss, test_accuracy = check_accuracy(model, loader_test, criterion, device)\n",
    "train_loss, train_accuracy = check_accuracy(model, loader_train_aug, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training with a pre-trained model and frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None # TODO: Load a model from torchvision.models and its weights\n",
    "# Freeze all the layers of the model except the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the first layer with a new layer that has the number of channels in the OrganSMNIST dataset e.g.\n",
    "#model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset e.g.\n",
    "#model.fc = nn.Linear(model.fc.in_features, 11)\n",
    "\n",
    "#TODO Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training with a pre-trained model and unfrozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None # TODO: Load a model from torchvision.models and its weights\n",
    "\n",
    "# Replace the first layer with a new layer that has the number of channels in the OrganSMNIST dataset e.g.\n",
    "#model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Replace the last layer with a new layer that has the number of classes in the OrganSMNIST dataset e.g.\n",
    "#model.fc = nn.Linear(model.fc.in_features, 11)\n",
    "\n",
    "#TODO Train the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
