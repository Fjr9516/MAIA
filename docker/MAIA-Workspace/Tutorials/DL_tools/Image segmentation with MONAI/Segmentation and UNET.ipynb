{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "In this Lab, we'll be diving deeper into practical implementations of segmentation techniques, building on the concepts you've learned in the lecture.\n",
    "\n",
    "### What is Image Segmentation?\n",
    "\n",
    "Image segmentation is a crucial task in medical image analysis. It involves partitioning an image into multiple segments or regions, each corresponding to a different anatomical structure or area of interest. Accurate segmentation is essential for various medical applications, including:\n",
    "\n",
    "- Tumor detection and measurement\n",
    "- Organ volume estimation\n",
    "- Surgical planning\n",
    "\n",
    "The idea of segmentation is that we classify each pixel as to whether it belongs to the region of interest or not. In the examples above the regions or interests are tumours, organs etc.\n",
    "\n",
    "![](brain_segmentation.png)\n",
    "\n",
    "## The Sliding Window Approach\n",
    "\n",
    "Before we dive into more advanced techniques, let's briefly recap the sliding window approach, which has been a traditional method for image segmentation:\n",
    "\n",
    "1. A fixed-size window \"slides\" across the image\n",
    "2. For each window position, a classifier predicts whether the central pixel belongs to the target segment\n",
    "3. This process is repeated for the entire image\n",
    "\n",
    "While simple to implement, the sliding window method has some limitations:\n",
    "- It can be computationally expensive for large images\n",
    "- It doesn't consider the full context of the image\n",
    "- The fixed window size may not be optimal for all structures\n",
    "\n",
    "In this lab we will not be using the sliding window approach but instead we will be using a more advanced technique called the UNET.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Randomizable,\n",
    "    Resized,\n",
    "    ScaleIntensityd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    Lambda\n",
    ")\n",
    "import os\n",
    "import tempfile\n",
    "from utils.decathlon_dataset import get_decathlon_dataloader\n",
    "from utils.unet import UNET\n",
    "from utils.train import train\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI Data Loading and Preprocessing for Hippocampus Segmentation\n",
    "\n",
    "The code in `utils/decathlon_dataset.py` demonstrates how to set up a data loading and preprocessing pipeline for the Hippocampus segmentation task using the MONAI library.\n",
    "\n",
    "1. **Directory Setup**: \n",
    "   - Establishes a root directory for data storage, either from an environment variable or a temporary directory.\n",
    "\n",
    "2. **Transform Pipeline**:\n",
    "   - Utilizes MONAI's `Compose` to create a sequence of transformations:\n",
    "     - `LoadImaged`: Loads image and label data.\n",
    "     - `EnsureChannelFirstd`: Ensures data has a channel dimension.\n",
    "     - `Orientationd`: Orients data to RAS (Right, Anterior, Superior) format.\n",
    "     - `Spacingd`: Resamples data to 1.0mm isotropic voxels.\n",
    "     - `ScaleIntensityd`: Scales image intensities.\n",
    "     - `Resized`: Resizes images and labels to 32x64x32 voxels.\n",
    "     - `EnsureTyped`: Ensures consistent data types.\n",
    "\n",
    "3. **Dataset Creation**:\n",
    "   - Uses `monai.apps.DecathlonDataset` to load the Hippocampus segmentation task data.\n",
    "   - Applies the defined transform pipeline to preprocess the data.\n",
    "\n",
    "4. **DataLoader Setup**:\n",
    "   - Creates a PyTorch DataLoader for efficient batch processing during training.\n",
    "   - Configures with a batch size of 4, shuffling, and 2 worker processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = './utils/datasets'\n",
    "task = \"Task04_Hippocampus\"\n",
    "\n",
    "train_loader = get_decathlon_dataloader(root_dir, task, \"training\", batch_size=4, num_workers=2, shuffle=True)\n",
    "val_loader = get_decathlon_dataloader(root_dir, task, \"validation\", batch_size=4, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting image examples\n",
    "\n",
    "Let's visualize a couple of image below. We extract a single slice from the images to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot sample slices\n",
    "def plot_sample_slices(batch, num_samples=4):\n",
    "    images = batch['image'].numpy()\n",
    "    labels = batch['label'].numpy()\n",
    "    print(images.shape)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))\n",
    "    for i in range(num_samples):\n",
    "        axes[0, i].imshow(images[i+20, 0,:, :], cmap='gray')\n",
    "        axes[0, i].set_title(f'Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(labels[i+20, 0,:,:], cmap='viridis')\n",
    "        axes[1, i].set_title(f'Label {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Verify the shape of the data and plot sample slices\n",
    "for batch in train_loader:\n",
    "    print(f\"Image shape: {batch['image'].shape}, Label shape: {batch['label'].shape}\")\n",
    "    plot_sample_slices(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing UNET:\n",
    "\n",
    "Now, we'll be implementing a more advanced and efficient architecture for medical image segmentation: the UNET. \n",
    "\n",
    "UNET, introduced by [Ronneberger et al. in 2015](https://arxiv.org/abs/1505.04597), is a convolutional neural network designed specifically for biomedical image segmentation. Many state-of-the-art segmentation models still build on this architecture. It has several advantages over the sliding window approach:\n",
    "\n",
    "- It considers the full image context\n",
    "- It's more efficient, requiring fewer training samples\n",
    "- It can handle varying sizes of target structures\n",
    "\n",
    "In this lab, you'll gain hands-on experience implementing the original UNET architecture, understanding its components, and applying it to medical imaging data.\n",
    "\n",
    "![](UNET_architecture.png)\n",
    "\n",
    "\n",
    "\n",
    "Go to the `utils/unet.py` file and follow the instructions to implement the UNET architecture there. Run the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.unet import UNET\n",
    "x = torch.randn((3, 1, 161, 161))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "preds = model(x)\n",
    "assert preds.shape == x.shape, f\"The output shape is not as expected got: {preds.shape} expected: {x.shape}\"\n",
    "print(\"UNET output shape is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. We will be using the training data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import train\n",
    "\n",
    "model = UNET(in_channels=1, out_channels=3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "trained_model = train(model, train_loader, loss_fn, optimizer, device=device, epochs=10)\n",
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), 'trained_unet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# Function to plot segmentation results\n",
    "def plot_segmentation_results(model, data_loader, num_samples=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            images = images.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            fig, axes = plt.subplots(3, num_samples, figsize=(20, 12))\n",
    "            for i in range(num_samples):\n",
    "                axes[0, i].imshow(images[20+i, 0, :, :], cmap='gray')\n",
    "                axes[0, i].set_title(f'Image {i+1}')\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                axes[1, i].imshow(labels[20+i, 0, :, :], cmap='viridis')\n",
    "                axes[1, i].set_title(f'True Label {i+1}')\n",
    "                axes[1, i].axis('off')\n",
    "                \n",
    "                axes[2, i].imshow(preds[20+i, :, :], cmap='viridis')\n",
    "                axes[2, i].set_title(f'Prediction {i+1}')\n",
    "                axes[2, i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "# Plot results for training data\n",
    "print(\"Training Data Results:\")\n",
    "plot_segmentation_results(trained_model, train_loader)\n",
    "\n",
    "# Plot results for validation data\n",
    "print(\"Validation Data Results:\")\n",
    "plot_segmentation_results(trained_model, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Segmentation Models: Beyond Accuracy\n",
    "\n",
    "We can hopefully see that the model is doing something that looks like it's segmenting the image even though it is far from perfect. The question is; however, how can we quantify it?  While accuracy is a common metric in classification tasks, it's often inadequate for segmentation problems. Here's why:\n",
    "\n",
    "1. **Class Imbalance**: In many segmentation tasks, the region of interest (e.g., a tumor) may occupy only a small portion of the image. A model that simply predicts \"background\" for every pixel could achieve high accuracy but fail to segment the target region.\n",
    "\n",
    "2. **Spatial Information**: Accuracy doesn't account for the spatial relationship between predicted and true segmentations. It treats each pixel independently, missing important context.\n",
    "\n",
    "3. **Boundary Precision**: Accuracy doesn't differentiate between minor and major segmentation errors, which can be crucial in medical applications.\n",
    "\n",
    "### Better Alternatives\n",
    "\n",
    "1. **Intersection over Union (IoU) / Jaccard Index**:\n",
    "   - Measures overlap between predicted and ground truth segmentations.\n",
    "   - Range: 0 to 1 (higher is better).\n",
    "   - Handles class imbalance well.\n",
    "\n",
    "2. **Dice Coefficient**:\n",
    "   - Similar to IoU but gives more weight to true positives.\n",
    "   - Range: 0 to 1 (higher is better).\n",
    "   - Widely used in medical image segmentation.\n",
    "\n",
    "3. **Mean IoU (mIoU)**:\n",
    "   - Average IoU across all classes.\n",
    "   - Useful for multi-class segmentation.\n",
    "\n",
    "4. **Hausdorff Distance**:\n",
    "   - Measures the maximum distance between predicted and true segmentation boundaries.\n",
    "   - Important for assessing boundary accuracy.\n",
    "\n",
    "These metrics provide a more comprehensive evaluation of segmentation performance, capturing aspects like overlap, boundary precision, and class balance that accuracy alone misses. Let's first looks at what happens when we use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "            correct += (preds == labels.squeeze(1)).sum().item()\n",
    "            total += labels.numel()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Calculate accuracy for the trained model\n",
    "model_accuracy = calculate_accuracy(trained_model, val_loader, device)\n",
    "\n",
    "# Calculate accuracy for predicting 0 everywhere\n",
    "def zero_prediction_accuracy(data_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in data_loader:\n",
    "        labels = batch['label'].to(device)\n",
    "        correct += (labels == 0).sum().item()\n",
    "        total += labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "zero_pred_accuracy = zero_prediction_accuracy(val_loader, device)\n",
    "\n",
    "print(f\"Trained model accuracy: {model_accuracy*100:.2f}%\")\n",
    "print(f\"Zero prediction accuracy: {zero_pred_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy is not a good metric to evaluate the performance of the model since a model that predicts the same thing everywhere will have a high accuracy and our trained model does only perform slightly better than this. Let's compute the mean IoU metric instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, num_classes, smooth=1e-6):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).float()\n",
    "        target_cls = (target == cls).float()\n",
    "        intersection = (pred_cls * target_cls).sum((1, 2))\n",
    "        union = pred_cls.sum((1, 2)) + target_cls.sum((1, 2)) - intersection\n",
    "        iou = (intersection + smooth) / (union + smooth)\n",
    "        ious.append(iou.mean())\n",
    "    return sum(ious) / len(ious)\n",
    "\n",
    "def calculate_model_iou(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            iou = calculate_iou(preds, labels.squeeze(1), num_classes)\n",
    "            total_iou += iou.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_iou / num_batches\n",
    "\n",
    "def calculate_zero_pred_iou(data_loader, device, num_classes):\n",
    "    total_iou = 0\n",
    "    num_batches = 0\n",
    "    for batch in data_loader:\n",
    "        labels = batch['label'].to(device)\n",
    "        zero_pred = torch.zeros_like(labels.squeeze(1))\n",
    "        iou = calculate_iou(zero_pred, labels.squeeze(1), num_classes)\n",
    "        total_iou += iou.item()\n",
    "        num_batches += 1\n",
    "    return total_iou / num_batches\n",
    "\n",
    "# Assuming 3 classes (background + 2 hippocampus regions)\n",
    "num_classes = 3\n",
    "\n",
    "# Calculate IoU for the trained model\n",
    "model_iou = calculate_model_iou(trained_model, val_loader, device, num_classes)\n",
    "\n",
    "# Calculate IoU for zero prediction\n",
    "zero_pred_iou = calculate_zero_pred_iou(val_loader, device, num_classes)\n",
    "\n",
    "train_iou = calculate_model_iou(trained_model, train_loader, device, num_classes)\n",
    "print(f\"Trained model mean IoU: {model_iou:.4f}\")\n",
    "print(f\"Zero prediction mean IoU: {zero_pred_iou:.4f}\")\n",
    "print(f\"Trained model mean IoU on training set: {train_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your UNET implementation is correct you should see that the IoU metric (> 0.85) is much better at quantifying the performance of the model compared to accuracy. In this case we include the background class in the calculation, but often it is excluded, that is essentially 66 % of the pixels are ignored in the calculation. In this case mean iou for the zero prediction would be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Coefficient for Image Segmentation\n",
    "\n",
    "The Dice coefficient is a widely used metric to evaluate the performance of image segmentation models. In the context of segmentation, it measures the overlap between the predicted segmentation mask and the ground truth mask. The Dice coefficient is defined as:\n",
    "\n",
    "$$Dice = \\frac{2|X \\cap Y|}{|X| + |Y|}$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the set of pixels in the predicted segmentation mask\n",
    "- $Y$ is the set of pixels in the ground truth mask\n",
    "- $|X \\cap Y|$ represents the number of pixels that are correctly segmented (true positives)\n",
    "- $|X|$ and $|Y|$ are the total number of pixels in the predicted and ground truth masks, respectively\n",
    "\n",
    "In practical terms:\n",
    "- A Dice score of 1 indicates perfect overlap (ideal segmentation)\n",
    "- A Dice score of 0 indicates no overlap (worst-case segmentation)\n",
    "\n",
    "The Dice coefficient is particularly useful in medical image segmentation because:\n",
    "1. It's sensitive to both over-segmentation and under-segmentation\n",
    "2. It handles class imbalance well, which is common in medical images where the region of interest may be small compared to the background\n",
    "3. It provides a single, interpretable value to assess segmentation quality\n",
    "\n",
    "When calculating the Dice coefficient for multi-class segmentation, it's often computed separately for each class and then averaged to get an overall score. We also average over the entire dataset to get a single score.  Please complete the DiceScore class in the `utils/metrics.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import DiceScore\n",
    "\n",
    "# Initialize DiceScore\n",
    "dice_metric = DiceScore()\n",
    "\n",
    "# Function to calculate Dice score for the model\n",
    "def calculate_dice_score(model, data_loader, device):\n",
    "    model.eval()\n",
    "    dice_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate Dice score for this batch\n",
    "            batch_dice = dice_metric.calculate(labels.squeeze(1), outputs)\n",
    "    \n",
    "    # Compute the final mean Dice score\n",
    "    return dice_metric.mean()\n",
    "\n",
    "# Calculate Dice score for the trained model\n",
    "model_dice = calculate_dice_score(trained_model, val_loader, device)\n",
    "\n",
    "print(f\"Trained model Dice score on Validation set: {model_dice:.4f}\")\n",
    "# Training set dice score\n",
    "model_dice = calculate_dice_score(trained_model, train_loader, device)\n",
    "print(f\"Trained model Dice score on Training set: {model_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "From these results, we can see the model is performing similarly on the training and validation set. What conclusions can you draw from this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
